<!DOCTYPE html><html lang="zh-Hans"><head><!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]--><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="author" content="呼哧"><meta name="description" content="K-Means(K-均值) -&amp;gt;聚类K-Means 是一种使用广泛的聚类的算法，将各种聚类子集内的所有数据样本的均值作为该聚类的代表点。算法主要思想：通过迭代把数据集划分为不同的类别，使得评价聚类性能的准则函数达到最优，从而使得生成的每个聚类类内紧凑，类间独立。由于每次都要计算所有的样本与每一个质心之间的距离，在大规模数据集上收敛速度较慢。算法思想及步骤设置 k ， 以及k个聚类中心$u_1"><meta name="keywords" content="math,Machine Learning"><meta property="og:type" content="article"><meta property="og:title" content="机器学习初识"><meta property="og:url" content="http://hu-chi.github.io/2018/03/19/machineLearning/Lesson1/index.html"><meta property="og:site_name" content="Play with huchi"><meta property="og:description" content="K-Means(K-均值) -&amp;gt;聚类K-Means 是一种使用广泛的聚类的算法，将各种聚类子集内的所有数据样本的均值作为该聚类的代表点。算法主要思想：通过迭代把数据集划分为不同的类别，使得评价聚类性能的准则函数达到最优，从而使得生成的每个聚类类内紧凑，类间独立。由于每次都要计算所有的样本与每一个质心之间的距离，在大规模数据集上收敛速度较慢。算法思想及步骤设置 k ， 以及k个聚类中心$u_1"><meta property="og:locale" content="zh-Hans"><meta property="og:updated_time" content="2018-03-19T11:10:12.264Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="机器学习初识"><meta name="twitter:description" content="K-Means(K-均值) -&amp;gt;聚类K-Means 是一种使用广泛的聚类的算法，将各种聚类子集内的所有数据样本的均值作为该聚类的代表点。算法主要思想：通过迭代把数据集划分为不同的类别，使得评价聚类性能的准则函数达到最优，从而使得生成的每个聚类类内紧凑，类间独立。由于每次都要计算所有的样本与每一个质心之间的距离，在大规模数据集上收敛速度较慢。算法思想及步骤设置 k ， 以及k个聚类中心$u_1"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="alternate" href="/atom.xml" title="Play with huchi" type="application/atom+xml"><link rel="shortcut icon" href="/img/huchi.jpg"><link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet"><link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet"><script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script><link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet"><link rel="stylesheet" href="/css/style.css"><link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet"><title>机器学习初识 | Play with huchi</title><script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script><script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script><script>var yiliaConfig={fancybox:!0,animate:!0,isHome:!1,isPost:!0,isArchive:!1,isTag:!1,isCategory:!1,fancybox_js:"//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",scrollreveal:"//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",search:!0}</script><script>yiliaConfig.jquery_ui=[!0,"//cdn.bootcss.com/jqueryui/1.10.4/jquery-ui.min.js","//cdn.bootcss.com/jqueryui/1.10.4/css/jquery-ui.min.css"]</script><script>yiliaConfig.rootUrl="/"</script></head></html><body><div id="container"><div class="left-col"><div class="overlay"></div><div class="intrude-less"><header id="header" class="inner"><a href="/" class="profilepic"><img src="/img/huchi.jpg" class="animated zoomIn"></a><hgroup><h1 class="header-author"><a href="/">呼哧</a></h1></hgroup><p class="header-subtitle">看遍南海风云卷，最恨难赴故人约。</p><form id="search-form"><input type="text" id="local-search-input" name="q" placeholder="search..." class="search form-control" autocomplete="off" autocorrect="off" searchonload="false"> <i class="fa fa-times" onclick="resetSearch()"></i></form><div id="local-search-result"></div><p class="no-result">No results found <i class="fa fa-spinner fa-pulse"></i></p><div id="switch-btn" class="switch-btn"><div class="icon"><div class="icon-ctn"><div class="icon-wrap icon-house" data-idx="0"><div class="birdhouse"></div><div class="birdhouse_holes"></div></div><div class="icon-wrap icon-ribbon hide" data-idx="1"><div class="ribbon"></div></div><div class="icon-wrap icon-link hide" data-idx="2"><div class="loopback_l"></div><div class="loopback_r"></div></div><div class="icon-wrap icon-me hide" data-idx="3"><div class="user"></div><div class="shoulder"></div></div></div></div><div class="tips-box hide"><div class="tips-arrow"></div><ul class="tips-inner"><li>菜单</li><li>标签</li><li>友情链接</li><li>关于我</li></ul></div></div><div id="switch-area" class="switch-area"><div class="switch-wrap"><section class="switch-part switch-part1"><nav class="header-menu"><ul><li><a href="/">主页</a></li><li><a href="/archives/">所有文章</a></li><li><a href="/tags/随笔">随笔</a></li><li><a href="/tags/">标签云</a></li><li><a href="/about/">关于我</a></li></ul></nav><nav class="header-nav"><ul class="social"><a class="fa Email" href="/779197837@qq.com" title="Email"></a> <a class="fa GitHub" href="#" title="GitHub"></a> <a class="fa RSS" href="/atom.xml" title="RSS"></a></ul></nav></section><section class="switch-part switch-part2"><div class="widget tagcloud" id="js-tagcloud"><ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hadoop/">Hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/">Linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/acm/">acm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/django/">django</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/js/">js</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/math/">math</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/script/">script</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/starts/">starts</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/前端设计/">前端设计</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/杂书/">杂书</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/网络编程/">网络编程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/设计模式/">设计模式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/随笔/">随笔</a></li></ul></div></section><section class="switch-part switch-part3"><div id="js-friends"><a class="main-nav-link switch-friends-link" href="https://hexo.io">Hexo</a> <a class="main-nav-link switch-friends-link" href="https://pages.github.com/">GitHub</a> <a class="main-nav-link switch-friends-link" href="http://moxfive.xyz/">MOxFIVE</a></div></section><section class="switch-part switch-part4"><div id="js-aboutme">专注于前端</div></section></div></div></header></div></div><div class="mid-col"><nav id="mobile-nav"><div class="overlay"><div class="slider-trigger"></div><h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">呼哧</a></h1></div><div class="intrude-less"><header id="header" class="inner"><a href="/" class="profilepic"><img src="/img/huchi.jpg" class="animated zoomIn"></a><hgroup><h1 class="header-author"><a href="/" title="回到主页">呼哧</a></h1></hgroup><p class="header-subtitle">看遍南海风云卷，最恨难赴故人约。</p><nav class="header-menu"><ul><li><a href="/">主页</a></li><li><a href="/archives/">所有文章</a></li><li><a href="/tags/随笔">随笔</a></li><li><a href="/tags/">标签云</a></li><li><a href="/about/">关于我</a></li><div class="clearfix"></div></ul></nav><nav class="header-nav"><ul class="social"><a class="fa Email" target="_blank" href="/779197837@qq.com" title="Email"></a> <a class="fa GitHub" target="_blank" href="#" title="GitHub"></a> <a class="fa RSS" target="_blank" href="/atom.xml" title="RSS"></a></ul></nav></header></div><link class="menu-list" tags="标签" friends="友情链接" about="关于我"></nav><div class="body-wrap"><article id="article-machineLearning/Lesson1" class="article article-type-article" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2018/03/19/machineLearning/Lesson1/" class="article-date"><time datetime="2018-03-19T11:40:42.000Z" itemprop="datePublished">2018-03-19</time></a></div><div class="article-inner"><input type="hidden" class="isFancy"><header class="article-header"><h1 class="article-title" itemprop="name">机器学习初识</h1></header><div class="article-info article-info-post"><div class="article-category tagcloud"><a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a></div><div class="article-tag tagcloud"><ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/math/">math</a></li></ul></div><div class="clearfix"></div></div><div class="article-entry" itemprop="articleBody"><h1 id="K-Means-K-均值-gt-聚类"><a href="#K-Means-K-均值-gt-聚类" class="headerlink" title="K-Means(K-均值) -&gt;聚类"></a>K-Means(K-均值) -&gt;聚类</h1><p>K-Means 是一种使用广泛的聚类的算法，将各种聚类子集内的所有数据样本的<strong>均值</strong>作为该聚类的代表点。</p><p>算法主要思想：通过迭代把数据集划分为不同的类别，使得评价聚类性能的准则函数达到最优，从而使得生成的每个聚类类内紧凑，类间独立。</p><p>由于每次都要计算所有的样本与每一个质心之间的距离，在大规模数据集上收敛速度较慢。</p><h2 id="算法思想及步骤"><a href="#算法思想及步骤" class="headerlink" title="算法思想及步骤"></a>算法思想及步骤</h2><ol><li>设置 k ， 以及k个聚类中心$u_1, u_2, …, u_k$</li><li>分组：<ul><li>将样本分配给距离最近的聚类中心</li><li>由这些样本构造不相交(non-overlapping)的聚类</li></ul></li><li>确定中心： 用个聚类的<strong>均值向量</strong>作为新的聚类中心</li><li>重复2，3直至算法收敛即聚类中心达到稳定。</li></ol><h2 id="算法要点"><a href="#算法要点" class="headerlink" title="算法要点"></a>算法要点</h2><ol><li><p>选定某种距离作为数据样本间的相似性度量</p><blockquote><p>k-means聚类算法不适合处理离散型属性，对连续性属性比较适合</p><p>计算样本之间的距离时，根据实际需要选择距离测度作为算法的相似性度量，如欧式距离，曼哈顿距离(各维差的绝对值之和)，马氏距离(两个服从同一分布并且其协方差矩阵为Σ的随机变量之间的差异程度)</p></blockquote></li><li><p>选择评价聚类性能的准则函数</p><blockquote><p>k-means聚类算法使用误差平方和准则函数来评价聚类性能。</p><p>$$cost·Function = \sum_{i=1}^{k}\sum_{x_j \in S_i }(x_j - u_i)^2$$</p></blockquote></li><li><p>相似性的计算根据一个簇中对象的平均值(均值向量)来进行</p></li></ol><a id="more"></a><h2 id="性能分析"><a href="#性能分析" class="headerlink" title="性能分析"></a>性能分析</h2><p><strong>Advantages:</strong></p><ul><li>解决聚类问题的经典算法，简单，快速</li><li>处理大数据集，相对可伸缩和高效率的，复杂度是O(nkt), k &lt;&lt;n , t &lt;&lt; n;</li><li>结果簇之间是密集的，而且簇与簇之间区别明显时，效果教好。</li></ul><p><strong>Disadvantages:</strong></p><ul><li>簇的平均值被定义的情况下才能使用，处理符号属性的数据不适用。</li><li>必须事先指定k</li><li>对初始值敏感，对于不同的初始值，可能会导致不同的结果</li><li>对于“噪声”和孤立数据点敏感，少量的该数据将产生极大的影响。</li></ul><h2 id="算法改进"><a href="#算法改进" class="headerlink" title="算法改进"></a>算法改进</h2><ol><li>对于离散型属性和符号型属性，度量相似度采用：<ul><li>比较记录，属性值相同为0，不相同为1，并取和。</li><li>更新新聚类中心，选择每个簇中的属性值出现频率最大的一个或几个作为代表簇的属性值。</li></ul></li><li>对于类别数k的指定，考虑类别的合并与分裂。<ul><li>合并： 某一类中样本太少，两类聚类中心距离过近</li><li>分裂：方差过大</li></ul></li><li>初始值敏感<ul><li>多设置不同的初始值，对比最后的结果，直至结果趋同。(耗时)</li></ul></li><li>对于“噪声”和孤立数据点<ul><li>不采用均值，采用<strong>中心点</strong></li><li>基于最小化所有对象与其参照点之间相异度之和的原则？？？</li></ul></li></ol><h2 id="算法实现-python"><a href="#算法实现-python" class="headerlink" title="算法实现(python)"></a>算法实现(python)</h2><p>导入数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">(fileName)</span>:</span></span><br><span class="line">  dataMat = []</span><br><span class="line">  <span class="keyword">with</span> open(fileName, <span class="string">'r'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fp.readlines():</span><br><span class="line">      curLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">      fltLine = [float(x) <span class="keyword">for</span> x <span class="keyword">in</span> curLine]</span><br><span class="line">      dataMat.append(fltLine)</span><br><span class="line">      <span class="keyword">return</span> mat(dataMat)</span><br></pre></td></tr></table></figure><p>生成k个点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distEclud</span><span class="params">(vecA, vecB)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> sqrt(sum(power(vecA - vecB, <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randCent</span><span class="params">(dataSet, k)</span>:</span></span><br><span class="line">  <span class="comment"># the dimension</span></span><br><span class="line">  n = shape(dataSet)[<span class="number">1</span>]</span><br><span class="line">  <span class="comment"># get the random center coordinates</span></span><br><span class="line">  centroids = mat(zeros((k, n)))</span><br><span class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">    minJ = min(dataSet[:, j])</span><br><span class="line">    rangeJ = float(max(dataSet[:, j]) - minJ)</span><br><span class="line">    centroids[:,j] = minJ + rangeJ * random.rand(k, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> centroids</span><br></pre></td></tr></table></figure><p>k-means:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kMeans</span><span class="params">(dataSet, k, distMeas=distEclud, createCent=randCent)</span>:</span></span><br><span class="line">  m = shape(dataSet)[<span class="number">0</span>]</span><br><span class="line">  clusterAssment = mat(zeros((m, <span class="number">2</span>)))</span><br><span class="line">  centroids = createCent(dataSet, k)</span><br><span class="line">  clusterChanged = <span class="keyword">True</span></span><br><span class="line">  <span class="keyword">while</span> clusterChanged:</span><br><span class="line">    clusterChanged = <span class="keyword">False</span></span><br><span class="line">    <span class="comment"># calcuate the distance</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">      minDist = inf</span><br><span class="line">      minIndex = <span class="number">-1</span></span><br><span class="line">      <span class="keyword">for</span> j <span class="keyword">in</span> range(k):</span><br><span class="line">        distJI = distMeas(centroids[j, :], dataSet[i, :])</span><br><span class="line">        <span class="keyword">if</span> distJI &lt; minDist:</span><br><span class="line">          minDist = distJI</span><br><span class="line">          minIndex = j</span><br><span class="line">          <span class="keyword">if</span> clusterAssment[i, <span class="number">0</span>] != minIndex:</span><br><span class="line">            <span class="comment"># unable to be stable</span></span><br><span class="line">            clusterChanged = <span class="keyword">True</span></span><br><span class="line">            clusterAssment[i, :] = minIndex, minDist**<span class="number">2</span></span><br><span class="line">            <span class="comment"># print(centroids)</span></span><br><span class="line">            <span class="comment"># update center coordinates</span></span><br><span class="line">            <span class="keyword">for</span> cent <span class="keyword">in</span> range(k):</span><br><span class="line">              ptsInClust = dataSet[nonzero(clusterAssment[:, <span class="number">0</span>].A  == cent)[<span class="number">0</span>]]</span><br><span class="line">              centroids[cent, :] = mean(ptsInClust, axis=<span class="number">0</span>)</span><br><span class="line">              <span class="keyword">return</span> centroids, clusterAssment</span><br></pre></td></tr></table></figure><p>画图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw</span><span class="params">(dataSet, centroids, clusterAssment)</span>:</span></span><br><span class="line">  m = shape(dataSet)[<span class="number">0</span>]</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">    plt.scatter(dataSet[i, <span class="number">0</span>], dataSet[i, <span class="number">1</span>], c=color(clusterAssment[i, <span class="number">0</span>]), )</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>主程序</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">  dataSet = loadDataSet(<span class="string">'places.txt'</span>)</span><br><span class="line">  centroids, clusterAssment = kMeans(dataSet, <span class="number">4</span>)</span><br><span class="line">  draw(dataSet, centroids, clusterAssment)</span><br></pre></td></tr></table></figure><h3 id="二分k-means优化"><a href="#二分k-means优化" class="headerlink" title="二分k-means优化"></a>二分k-means优化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">biKmeans</span><span class="params">(dataSet, k, distMeas=distEclud)</span>:</span></span><br><span class="line">  <span class="comment"># number of dataSet</span></span><br><span class="line">  m = shape(dataSet)[<span class="number">0</span>]</span><br><span class="line">  clusterAssment = mat(zeros((m, <span class="number">2</span>)))</span><br><span class="line">  <span class="comment"># the first of center coordinate</span></span><br><span class="line">  centroidOne = mean(dataSet, axis=<span class="number">0</span>).tolist()[<span class="number">0</span>]</span><br><span class="line">  centList = [centroidOne]</span><br><span class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">    <span class="comment"># SSE of the first center coordiante and every one of dataset</span></span><br><span class="line">    clusterAssment[j, <span class="number">1</span>] = distMeas(mat(centroidOne), dataSet[j, :])**<span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> (len (centList) &lt; k):</span><br><span class="line">      lowestSSE = inf</span><br><span class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> range(len(centList)):</span><br><span class="line">        <span class="comment"># 第 i 类的数据集</span></span><br><span class="line">        ptsInCurrClust = dataSet[nonzero(clusterAssment[:, <span class="number">0</span>].A == i)[<span class="number">0</span>], :]</span><br><span class="line">        centroidMat, splitClustAss = kMeans(ptsInCurrClust, <span class="number">2</span>, distMeas)</span><br><span class="line">        <span class="comment"># calculate the SSE</span></span><br><span class="line">        sseSplit = sum(splitClustAss[:, <span class="number">1</span>])</span><br><span class="line">        sseNotSplit = sum(clusterAssment[nonzero(clusterAssment[:, <span class="number">0</span>].A != i)[<span class="number">0</span>], <span class="number">1</span>])</span><br><span class="line">        print(<span class="string">"SSE of split : %d \nSSE of nosplit : %d"</span> % (sseSplit, sseNotSplit))</span><br><span class="line">        <span class="keyword">if</span> (sseSplit + sseNotSplit) &lt; lowestSSE:</span><br><span class="line">          bestCentToSpit = i</span><br><span class="line">          bestNewCents = centroidMat</span><br><span class="line">          bestClustAss = splitClustAss.copy()</span><br><span class="line">          lowestSSE = sseSplit + sseNotSplit</span><br><span class="line"></span><br><span class="line">          <span class="comment"># divide it </span></span><br><span class="line">          bestClustAss[nonzero(bestClustAss[:, <span class="number">0</span>].A == <span class="number">1</span>)[<span class="number">0</span>], <span class="number">0</span>] = len(centList)</span><br><span class="line">          bestClustAss[nonzero(bestClustAss[:, <span class="number">0</span>].A == <span class="number">0</span>)[<span class="number">0</span>], <span class="number">0</span>] = bestCentToSpit</span><br><span class="line"></span><br><span class="line">          print(<span class="string">"The bestCentToSpit is %d "</span> % (bestCentToSpit))</span><br><span class="line">          print(<span class="string">"The len of bestClustAss is %d "</span> % (len(centList)))</span><br><span class="line">          <span class="comment"># update and add center coordinate</span></span><br><span class="line">          centList[bestCentToSpit] = bestNewCents[<span class="number">0</span>, :]</span><br><span class="line">          centList.append(bestNewCents[<span class="number">1</span>, :])</span><br><span class="line">          <span class="comment"># change the class which is divided into 2 parts</span></span><br><span class="line">          clusterAssment[nonzero(clusterAssment[:, <span class="number">0</span>].A == bestCentToSpit)[<span class="number">0</span>], :] = bestClustAss</span><br><span class="line"></span><br><span class="line">          <span class="keyword">return</span> centList, clusterAssment</span><br></pre></td></tr></table></figure><p>经过几次测试，发现不如不优化。这个是通过我每次去找一个当前最优解。</p><h1 id="KNN-近邻法"><a href="#KNN-近邻法" class="headerlink" title="KNN(近邻法)"></a>KNN(近邻法)</h1><p><strong>最近邻</strong></p><p>设有c个类$ \omega_1, \omega_2,…, \omega_c $,每类有$N_i$个样本。</p><p>待测样本的到第i类的最近距离： $g_i(x) = min|| x - x_i^k || \ \ \ (x_i^k表示的是第i类第k个样本，k=1,…N_i)$</p><p>距离我们除了可以采用欧式距离和曼哈顿距离，还可以采用明考斯基距离。</p><p>名考夫斯基距离：</p><p>$$d(a,b) = (\sum_{i = 1}^{n}|x_{a_i} - x_{b_i}|^p)^{\frac{1}{p}}$$</p><p>加权名考夫斯基距离：</p><p>$$d(a,b) = (\sum_{i = 1}^{n}w_i·|x_{a_i} - x_{b_i}|^p)^{\frac{1}{p}}$$</p><p><strong>决策规则</strong></p><p>把待测样本加g(x)最小的那个类中。</p><p>k = 1时称为最近邻分类器。</p><p>给定测试样本，若其最近邻样本为z,则最近邻分类器出错的概率就是x与z类别标记不同的概率</p><p>$$ P(error) = 1 - \sum _{c \in \gamma}P(c | x) P(c|z)$$</p><p>最近邻分类器虽简单，但是他的泛化错误率不超过贝叶斯最优分类器的错误率的两倍。</p><p><strong>低维嵌入</strong></p><p>当训练样本的采集密度足够大，成为”密采样(dense sample)”,但随着维度的提升，样本数目明显不够，导致数据样本稀疏，距离计算困难等难题，即“维数灾难(curse of dimensionality)”。</p><p>缓解的一个重要途径就是<strong>降维</strong>(dimension reduction).人们观测数据样本虽是高维的，但是与学习任务密切相关的也许尽是某个低维分布。</p><p><strong>典型的多维缩放</strong>(Multiple Dimensional Scaling , MDS):</p><p>假定m个样本在原始控件的距离矩阵为$D \in R^{m × m}, dist_{ij}$表示$x_i$到$x_j$的距离，目标是获取d’维空间的表示$Z \in R^{d’ × m}$, 且任意两个样本在d’维空间中的欧氏距离等于原始空间中的距离。</p><p>令$B = Z^{T}Z \in R^{m ×m}$， 其中B为降维后样本的内积矩阵，有$b_{ij} = z^{T}_iz_j$</p><p>$$dist_{ij}^{2} = ||z_i||^2 + ||z_j||^2 - 2z^T_iz_j = b_{ii} + b_{jj} - 2 b_{ij}$$</p><p>我们将为后的样本Z做中心化处理，也就是说$\sum_{i=1}^m z_i = 0$,易知</p><p>$$\sum_{i=1}^m dist^2_{ij} = tr(B) + m b_{jj}$$</p><p>$$\sum_{j=1}^m dist^2_{ij} = tr(B) + m b_{ii}$$</p><p>$$\sum_{i=1}^m\sum_{j=1}^m dist_{ij}^2 = 2m tr(B) $$</p><p>其中tr(B)表示B的迹，而且有$tr(B = \sum_{i = 1}^m ||z_i||^2$，我们再令</p><p>$$dist_{i·}^2 = \frac{1}{m}\sum_{j=1}^mdist_{ij}^2$$</p><p>$$dist_{·j}^2 = \frac{1}{m}\sum_{i=1}^mdist_{ij}^2$$</p><p>$$dist_{··}^2 = \frac{1}{m^2}\sum_{i=1}^m\sum_{j=1}^mdist_{ij}^2$$</p><p>所以我们有$b_{ij} = -\frac{1}{2}(dist_{ij}^2 - dist_{i·}^2 - dist_{·j}^2 + dist_{··}^2)$</p><p>通过降维前后保持不变的距离矩阵D来求取内积矩阵B.</p><p>再对B做特征值分解(eigenvalue decomposition), $B = V\Lambda V^T$, 其中$\Lambda = diag(\lambda_1, …, \lambda_d) ,(\lambda_1 \ge \lambda_2 \ge …\ge \lambda_d)$为特征值构成的对角矩阵.假定其中有d*个非零特征向值，他们构成了$\Lambda^{<em>} = diag(\lambda_1, lambda_2, …, \lambda_{d^{</em>}}), 令V^{<em>}$ 为相应的特征向量矩阵，则Z可表达为$Z = \Lambda^{1/2}_{</em>}V_{*}^{T}$</p><p>现实中为了有效降维，往往仅需降维后的距离与原始空间中的距离尽可能的接近。</p><p>MDS算法的过程：</p><ol><li>计算$dist_{ij}^{2},dist_{i· }^{2},dist_{j·}^{2}$</li><li>计算B</li><li>对B做特征值分解</li><li>取$\Lambda<em>为d’个最大特征值所构成的对角矩阵，V^{</em>}$为相应得到特征向量矩阵</li><li>得到的Z，每行便是一个样本的低维坐标</li></ol><h2 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h2><ol><li>计算出样本数据和待分类数据的距离，</li><li>为待分类数据选择k个与其距离最小的样本。</li><li>统计出k个样本中大多数样本所属的分类</li><li>这个分类就是待测数据所属的分类。</li></ol><h2 id="性能分析-1"><a href="#性能分析-1" class="headerlink" title="性能分析"></a>性能分析</h2><p><strong>Advantages:</strong></p><p>KNN不仅可以用来分类，也可以用来<a href="http://www.saedsayad.com/k_nearest_neighbors_reg.htm" target="_blank" rel="noopener">regression</a>。</p><p>对于类域交叉或重叠较多的待测样本集来说，非常适合。</p><p><strong>Disadvantages:</strong></p><p>当样本不平衡时，会导致待测样本偏向于样本容量较大的类。</p><p>计算量大(可除去对分类作用不大的样本)。</p><p>不适合样本容量较小的类域，容易采用误分。</p><p><strong>Others:</strong></p><p>k值的减小会使<a href="https://en.wikipedia.org/wiki/Approximation_error" target="_blank" rel="noopener">近似误差(approximation error)减小</a>,但<a href="https://www.zhihu.com/question/60793482" target="_blank" rel="noopener">估计误差(estimation error)</a>会增大，意味着整体模型变得复杂，容易发生过拟合。</p><p>k值的增大可以减小估计误差，但增大了近似误差。这是与输入实例较远的(不相似)训练实例也会起预测作用，是预测发生错误，同样也使得模型变的简单。</p><p>k=N不可取。k一般取一个较小的数值，采用交叉验证法来选取最优的k值。</p><h2 id="算法改进-1"><a href="#算法改进-1" class="headerlink" title="算法改进"></a>算法改进</h2><p><a href="http://blog.csdn.net/hit_buxiaoyu/article/details/53783269" target="_blank" rel="noopener">参考</a></p><p><strong>压缩kNN</strong></p><p>定义两个存储器，一个存放生成的样本集称output样本集，和original样本集。</p><p>初始化：output为空，original为原样本集，从original选择一个移到output中。</p><p>在original样本集中选择第i个样本，并使用output样本集中的样本对其进行knn，若分类错误，则将该样本移动到output样本中，若正确不做处理</p><p>直至遍历完original所有的样本。output也就是压缩后的样本集。</p><h2 id="算法实现-python-1"><a href="#算法实现-python-1" class="headerlink" title="算法实现(python    )"></a>算法实现(python )</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distEclud</span><span class="params">(vecA, vecB)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> sqrt(sum(power(vecA - vecB, <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classfiy0</span><span class="params">(inX, dataSet, labels, k)</span>:</span></span><br><span class="line">	m = shape(dataSet)[<span class="number">0</span>]</span><br><span class="line">	cnt = zeros(m)</span><br><span class="line">	dis = []</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">		tmp = distEclud(inX, dataSet[i, :])</span><br><span class="line">		dis.append((tmp, labels[i]))</span><br><span class="line">	dis.sort()</span><br><span class="line">	print(dis)</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">		cnt[dis[i][<span class="number">1</span>]] += <span class="number">1</span></span><br><span class="line">	id = <span class="number">-1</span></span><br><span class="line">	mix = <span class="number">0</span></span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">		<span class="keyword">if</span>(mix &lt; cnt[i]):</span><br><span class="line">			mix = cnt[i]</span><br><span class="line">			id = i</span><br><span class="line">	<span class="keyword">return</span> id</span><br></pre></td></tr></table></figure><h2 id="kd-Tree实现-c"><a href="#kd-Tree实现-c" class="headerlink" title="kd Tree实现(c++)"></a>kd Tree实现(c++)</h2><blockquote><p>待补</p></blockquote><h1 id="principal-component-analysis-PCA主成分分析"><a href="#principal-component-analysis-PCA主成分分析" class="headerlink" title="principal component analysis(PCA主成分分析)"></a>principal component analysis(PCA主成分分析)</h1><p>线性方法：对特征做线性组合，将高维数据投影到低维空间。</p><p>目的：寻找在最小均方意义下最能够代表原始数据的投影方法</p><p>我们需要寻找一个满足<strong>最近重构性</strong>和<strong>最大可分性</strong>的超平面。</p><p><strong>最近重构性</strong>:样本点到这个超平面的距离都足够近</p><p><strong>最大可分性</strong>:样本点到这个超平面上的投影能尽可能的分开</p><p><strong>最近重构性</strong></p><p>假定数据样本进行了中心化，即$\sum x_i = 0$,再假定投影变换后得到的新坐标系为${w_1, w_2, …,w_d}$, 且为规范正交基。若丢弃一部分坐标，维度降低到d’ &lt; d，则样本点$x_i$在低维坐标系下中的投影是$z_i = (z_{i1}; z_{i2}; …;z_{id’ })$, 其中$z_{ij}=w_{j}^{T}x_i$是$x_i$在低维下第j维的坐标。若基于$z_i$来重构$x_i$，则有$x_i = \sum_{j=1}^{d’}z_{ij}w_j$, 那么整个训练集中，样本原点与基于投影重构的样本点$x_i$之间的距离为:<br>$$<br>\sum_{i = 1}^{m}||\sum_{j=1}^{d’}z_{ij}w_j - x_i||<em>{2}^{2} = \sum</em>{i=1}^{m}z_i^{T}z_i - 2\sum_{i=1}^mz_i^TW^Tx_i + \sum x_i^2<br>$$<br>正比于$-tr(W^TXX^TW),s.t.W^TW = I$,</p><p>这就是主成分分析的优化目标。</p><p>使用拉格朗日乘子法得<br>$$<br>XX^Tw_i = \lambda_iw_i<br>$$<br>对于协方差矩阵$XX^T$ 进行特征值分解,可以得到<br>$$<br>E^\mathsf{T}CE=\Lambda=\begin{pmatrix}<br>\lambda_1 &amp; &amp; &amp; \<br>&amp; \lambda_2 &amp; &amp; \<br>&amp; &amp; \ddots &amp; \<br>&amp; &amp; &amp; \lambda_n<br>\end{pmatrix}<br>$$<br>其中特征值从大到下，再取前d’个特征值对应的特征向量构成W*, 这就是主成成分分析的解。</p><p><a href="http://blog.codinglabs.org/articles/pca-tutorial.html" target="_blank" rel="noopener">参考</a></p><p>降维后低维控件的维数d’通常是事先指定的，或通过在d’值不同的地位控件对knn(或其他开销较小的学习器)进行交叉验证来选取较好的d’值。对PCA还可以从重构的角度设置一个重构阈值，例如t = 95%, 然后选取满足不等式最小的d’<br>$$<br>\frac{\sum_{i=1}^{d’}\lambda_i}{\sum_{i=1}^{d}\lambda_i} \ge t<br>$$<br>舍弃了d-d’个特征值的特征向量，可能使样本的采集密度增大，另一方面当数据受到噪音影响时，最小的特征值所对应的特征向量往往与噪声有关。</p><p>PCA是一种无参数技术，也就是说面对同样的数据，如果不考虑清洗，谁来做结果都一样，没有主观参数的介入，所以PCA便于通用实现，但是本身无法个性化的优化。</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>输入m条n列的数据，低维空间维数d’</p><ol><li>对数据进行中心化处理，$x_i = x_i - \frac{1}{m} \sum_{i=1}^mx_i$</li><li>计算样本的协方差矩阵$XX^T$</li><li>对协方差最特征值分解，求出特征值以及特征向量</li><li>取最大的d’ 个特征值所对应的特征向量$w_1, w_2, w_3..$</li></ol><p>输出投影矩阵$W^* = (w_1, w_2, .., w_{d’})$</p><h2 id="python实现"><a href="#python实现" class="headerlink" title="python实现"></a>python实现</h2><p>PCA实现，输入m条n列(维)数据和d’，输出的是降维后的坐标，降维恢复后的坐标，以及投影矩阵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pca</span><span class="params">(dataSet, topFeat=<span class="number">1627406066</span>)</span>:</span></span><br><span class="line">	<span class="string">"""</span></span><br><span class="line"><span class="string">		return the data in low Dimensions and the recover data from that</span></span><br><span class="line"><span class="string">	"""</span></span><br><span class="line">	meanVals = mean(dataSet, axis=<span class="number">0</span>)</span><br><span class="line">	meanRemoved = dataSet - meanVals</span><br><span class="line">	covMat = cov(meanRemoved, rowvar=<span class="keyword">False</span>)</span><br><span class="line">	eigVals, eigVects = linalg.eig(mat(covMat))</span><br><span class="line">	eigValIndex = argsort(eigVals)</span><br><span class="line">	eigValIndex = eigValIndex[:-(topFeat+<span class="number">1</span>):<span class="number">-1</span>]</span><br><span class="line">	sortedEigVects = eigVects[:,eigValIndex]</span><br><span class="line">	lowData = meanRemoved * sortedEigVects</span><br><span class="line">	reData = (lowData * sortedEigVects.T) + meanVals</span><br><span class="line">	<span class="keyword">return</span> lowData, reData, sortedEigVects</span><br></pre></td></tr></table></figure><h1 id="Linear-Discriminant-Analysis-LDA线性判别分析"><a href="#Linear-Discriminant-Analysis-LDA线性判别分析" class="headerlink" title="Linear Discriminant Analysis(LDA线性判别分析)"></a>Linear Discriminant Analysis(LDA线性判别分析)</h1><p>思想：给定训练集，设法将样例投影到一条直线上，是的同类样例的投影点尽可能的接近、异类样例的投影点尽量远离，在对新样本进行分类时，将其投影到同样的这条直线上，在根据投影点到为止来确定样本的类别。</p><p><a href="http://dataunion.org/wp-content/uploads/2015/03/0_1321843208aGNj.gif" target="_blank" rel="noopener"></a></p><p>这个与PCA的想法相似但又截然不同。</p><p>给定数据集$D={ (x_i,y_i) }, y_i \in {0, 1}$. 令$X_i, \mu_i, \sum_i$分别表示第i类示例的集合、均值向量、协方差矩阵。若将数据投影到直线w上，则两类样本的中心在直线上的投影分别为$w^T \mu_0和w^T \mu_1$, 若将数据投影到直线w上。</p><p>欲使同类样例的投影点尽可能的接近，让同类的协方差尽可能的小，即$ w^T \sum_0w + w^T\sum_1w $, 而欲使异类样例的投影点尽可能原理,即$||w^T \mu_0 - w^T \mu_1||_2^2$尽可能大， 最大化的目标：<br>$$<br>J = \frac{||w^T \mu_0 - w^T \mu_1||_2^2}{ w^T \sum_0w + w^T\sum_1w } \ = \frac{w^T(\mu_0 - \mu_1)(\mu_0+\mu_1)^Tw}{w^T(\sum_0 + \sum_1)w}<br>$$<br>类内散度矩阵(within-class scatter matrix)</p><p>$S_w = \sum_0 + \sum_1$</p><p>$$= \sum_{x \in X_0} (x - u_0)(x - u_0)^T + \sum_{x \in X_1}(x - \mu_1)(x - \mu_1)^T​$$</p><p>类间散度矩阵(between-class scatter matrix):<br>$$<br>S_b = (\mu_0 - \mu_1)(\mu_0 - \mu_1)^T<br>$$</p><p>$$<br>J = \frac{w^TS_Bw}{w^TS_ww}<br>$$</p><p>这就是我们要最大化的目标，即$S_b 和S_w$的广义瑞利商(generalized Rayleigh quotient).</p><p>不失一般性，我们令分母为1，那么就有</p><p>$$J = w^TS_bw \ \ \ \ s.t. \ \ w^TS_ww = 1$$</p><p>拉格朗日乘子得：<br>$$<br>S_b w = \lambda S_w w<br>$$<br>这时$S_bw的方向与(\mu_0 - \mu_1)$一样，就可以令</p><p>$S_bw = \lambda (\mu_0 - \mu_1)$带回去就有</p><p>$$w = S_w^{-1} (\mu_0 - \mu_1)$$</p><p>考虑数值解的稳定性，通常将$S_w$进行奇异值分解(SVD)，即$S_w = U\sum V^T, S_w^{-1} = V\sum^{-1}U^T$</p><p><strong>当两类数据同先验，满足高斯分布且协方差相等时，LDA可达到最优分类</strong></p><p>推广：</p><p>存在N个类,且第i个类示例数为mi,定义全局散度矩阵<br>$$<br>S_t = S_b + S_w = \sum_{i=1}^{m}(x_i - \mu)(x_i - \mu)^T \<br>S_w = \sum_{i = 1}^N S_{w_i} \<br>S_{w_i} = \sum_{x \in X_i}(x - \mu_i) (x - \mu_i)^T \<br>S_b = S_t - S_w = \sum_{i = 1}^N (u_i - u)(u_i-u)^T<br>$$<br>多分类LDA有多种实现方法，使用$S_b, S_w, S_t$三者中任何两个即可，常见的是采用优化目标</p><p>$$\underset{W}{max} \frac{tr(W^TS_bW)}{tr(W^TS_wW) } $$</p><p>其中$W \in R^{d×(N-1)}$</p><p>上式同理可以转化为$S_bW = \lambda S_w W$</p><p>W的闭式解则是$S_w^{-1}S_b$的d’个最大非零广义特征值所对应的特征向量所组成的矩阵,$d’ \le N - 1$</p><p>W投影到d‘维空间，LDA也被视为一种经典的监督降维技术。</p><h2 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h2><p>输入m条n列的数据，</p><ol><li>计算$S_w,S_b$</li><li>计算矩阵$S_w^{-1}S_b$ 所有特征值和对应的特征向量</li><li>取最大的d’ 个特征值所对应的特征向量$w_1, w_2, w_3..$</li></ol><p>输出投影矩阵$W^* = (w_1, w_2, .., w_{d’})$</p><h2 id="python算法实现"><a href="#python算法实现" class="headerlink" title="python算法实现"></a>python算法实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lda</span><span class="params">(dataSet, label, topFeat=<span class="number">1627406066</span>)</span>:</span></span><br><span class="line">	<span class="string">"""</span></span><br><span class="line"><span class="string">		return the data in low Dimensions and the recover data from that</span></span><br><span class="line"><span class="string">	"""</span></span><br><span class="line">	m, n = shape(dataSet)</span><br><span class="line">	d = int(max(label)) + <span class="number">1</span></span><br><span class="line">	classifedData = [[] <span class="keyword">for</span> i <span class="keyword">in</span> range(d)]</span><br><span class="line">	labelAverage = []</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">		<span class="keyword">if</span>(int(label[i]) &lt; len(classifedData)):</span><br><span class="line">			classifedData[int(label[i])].append(dataSet[i])</span><br><span class="line">		<span class="keyword">else</span> :</span><br><span class="line">			print(<span class="string">"Error label : "</span> + label[i])</span><br><span class="line">	withinScatterMat = zeros((n, n))</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(d):</span><br><span class="line">		labelAverage.append(mean(classifedData[i], axis=<span class="number">0</span>))</span><br><span class="line">		tmp = mat(classifedData[i] - labelAverage[i])</span><br><span class="line">		withinScatterMat = withinScatterMat + tmp.T * tmp</span><br><span class="line">	</span><br><span class="line">	meanVals = mean(dataSet, axis=<span class="number">0</span>)</span><br><span class="line">	betweenScatterMat = zeros((n, n))</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(d):</span><br><span class="line">		tmp = mat(labelAverage[i] - meanVals)</span><br><span class="line">		betweenScatterMat += tmp.T * tmp</span><br><span class="line">	ScatterMat = (withinScatterMat ** (<span class="number">-1</span>)) * betweenScatterMat</span><br><span class="line">	eigVals, eigVects = linalg.eig(mat(ScatterMat))</span><br><span class="line">	print(eigVals)</span><br><span class="line">	eigValIndex = argsort(eigVals)</span><br><span class="line">	eigValIndex = [i <span class="keyword">for</span> i <span class="keyword">in</span> eigValIndex[:-(topFeat+<span class="number">1</span>):<span class="number">-1</span>] <span class="keyword">if</span>(i &gt; <span class="number">0</span>)]</span><br><span class="line">	sortedEigVects = eigVects[:,eigValIndex]</span><br><span class="line">	loadDataSet = dataSet * sortedEigVects</span><br><span class="line">	reData = loadDataSet * sortedEigVects.T</span><br><span class="line">	<span class="keyword">return</span> loadDataSet, reData, sortedEigVects</span><br></pre></td></tr></table></figure></div></div><div class="copyright"><p><span>本文标题:</span><a href="/2018/03/19/machineLearning/Lesson1/">机器学习初识</a></p><p><span>文章作者:</span><a href="/" title="回到主页">呼哧</a></p><p><span>发布时间:</span>2018-03-19, 19:40:42</p><p><span>最后更新:</span>2018-03-19, 19:10:12</p><p><span>原始链接:</span><a class="post-url" href="/2018/03/19/machineLearning/Lesson1/" title="机器学习初识">http://hu-chi.github.io/2018/03/19/machineLearning/Lesson1/</a> <span class="copy-path" data-clipboard-text="原文: http://hu-chi.github.io/2018/03/19/machineLearning/Lesson1/　　作者: 呼哧" title="点击复制文章链接"><i class="fa fa-clipboard"></i></span><script>var clipboard=new Clipboard(".copy-path")</script></p><p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="CC BY-NC-SA 4.0 International" target="_blank">"署名-非商用-相同方式共享 4.0"</a> 转载请保留原文链接及作者。</p></div><nav id="article-nav"><div id="article-nav-newer" class="article-nav-title"><a href="/2018/08/03/hadoop/Linux安装hadoop/">Hadoop的安装</a></div><div id="article-nav-older" class="article-nav-title"><a href="/2017/12/29/随笔集/2017年末总结/">2017年末总结</a></div></nav></article><div id="toc" class="toc-article"><strong class="toc-title">文章目录</strong><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#K-Means-K-均值-gt-聚类"><span class="toc-number">1.</span> <span class="toc-text">K-Means(K-均值) -&gt;聚类</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#算法思想及步骤"><span class="toc-number">1.1.</span> <span class="toc-text">算法思想及步骤</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#算法要点"><span class="toc-number">1.2.</span> <span class="toc-text">算法要点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#性能分析"><span class="toc-number">1.3.</span> <span class="toc-text">性能分析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#算法改进"><span class="toc-number">1.4.</span> <span class="toc-text">算法改进</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#算法实现-python"><span class="toc-number">1.5.</span> <span class="toc-text">算法实现(python)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#二分k-means优化"><span class="toc-number">1.5.1.</span> <span class="toc-text">二分k-means优化</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#KNN-近邻法"><span class="toc-number">2.</span> <span class="toc-text">KNN(近邻法)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#算法步骤"><span class="toc-number">2.1.</span> <span class="toc-text">算法步骤</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#性能分析-1"><span class="toc-number">2.2.</span> <span class="toc-text">性能分析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#算法改进-1"><span class="toc-number">2.3.</span> <span class="toc-text">算法改进</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#算法实现-python-1"><span class="toc-number">2.4.</span> <span class="toc-text">算法实现(python )</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#kd-Tree实现-c"><span class="toc-number">2.5.</span> <span class="toc-text">kd Tree实现(c++)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#principal-component-analysis-PCA主成分分析"><span class="toc-number">3.</span> <span class="toc-text">principal component analysis(PCA主成分分析)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#算法"><span class="toc-number">3.1.</span> <span class="toc-text">算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#python实现"><span class="toc-number">3.2.</span> <span class="toc-text">python实现</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Linear-Discriminant-Analysis-LDA线性判别分析"><span class="toc-number">4.</span> <span class="toc-text">Linear Discriminant Analysis(LDA线性判别分析)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#算法-1"><span class="toc-number">4.1.</span> <span class="toc-text">算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#python算法实现"><span class="toc-number">4.2.</span> <span class="toc-text">python算法实现</span></a></li></ol></li></ol></div><style>.left-col .switch-area,.left-col .switch-btn{display:none}.toc-level-3 i,.toc-level-3 ol{display:none!important}</style><input type="button" id="tocButton" value="隐藏目录" title="点击按钮隐藏或者显示文章目录"><script>yiliaConfig.toc=["隐藏目录","显示目录",!0]</script><div class="share"><div class="bdsharebuttonbox"><a href="#" class="fa fa-twitter bds_twi" data-cmd="twi" title="分享到推特"></a> <a href="#" class="fa fa-weibo bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a> <a href="#" class="fa fa-qq bds_sqq" data-cmd="sqq" title="分享给 QQ 好友"></a> <a href="#" class="fa fa-files-o bds_copy" data-cmd="copy" title="复制网址"></a> <a href="#" class="fa fa fa-envelope-o bds_mail" data-cmd="mail" title="通过邮件分享"></a> <a href="#" class="fa fa-weixin bds_weixin" data-cmd="weixin" title="生成文章二维码"></a> <a href="#" class="fa fa-share-alt bds_more" data-cmd="more"></a></div><script>with(window._bd_share_config={common:{bdSnsKey:{},bdText:"机器学习初识　| Play with huchi　",bdMini:"2",bdMiniList:!1,bdPic:"",bdStyle:"0",bdSize:"24"},share:{}},document)(getElementsByTagName("head")[0]||body).appendChild(createElement("script")).src="http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion="+~(-new Date/36e5)</script></div><div class="scroll" id="post-nav-button"><a href="/2018/08/03/hadoop/Linux安装hadoop/" title="上一篇: Hadoop的安装"><i class="fa fa-angle-left"></i> </a><a title="文章列表"><i class="fa fa-bars"></i><i class="fa fa-times"></i></a> <a href="/2017/12/29/随笔集/2017年末总结/" title="下一篇: 2017年末总结"><i class="fa fa-angle-right"></i></a></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/08/04/acm/hash/">hash在ACM种的应用</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/08/03/hadoop/wordcount/">Hadoop的使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/08/03/hadoop/Linux安装hadoop/">Hadoop的安装</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/19/machineLearning/Lesson1/">机器学习初识</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/29/随笔集/2017年末总结/">2017年末总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/21/设计模式/python设计模式(4)/">python设计模式(四)</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/20/设计模式/python设计模式(3)/">python设计模式(三)</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/19/设计模式/python设计模式(2)/">python设计模式(二)</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/24/设计模式/python设计模式(1)/">python设计模式(一)</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/12/fluentPython/fluent_python2/">fluent python (二)python数据模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/12/fluentPython/fluent_python5/">fluent python (五)一等函数</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/08/acm/循环节/">指数循环节</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/08/fluentPython/fluent_python1/">fluent python (一)python数据模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/08/acm/01分数规划/">01分数规划</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/17/acm/大数模板/">大数模版</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/17/acm/堆排序/">堆排序</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/17/acm/容斥原理/">容斥原理</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/10/acm/三分与退火模拟/">三分与模拟退火</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/05/acm/Acdreamer博客数论学习(2)/">Acdreamer博客数论学习(2)</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/04/acm/从dp的全世界路过(1)/">从dp的全世界路过(1)</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/20/acm/低等数论1/">数论知识点摘要1</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/19/acm/高斯消元/">高斯消元</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/19/acm/Acdreamer博客数论学习(1)/">Acdreamer博客数论学习(1)</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/16/acm/数论基础专题整理(1)/">数论专题整理(1)</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/12/acm/数位dp/">数位dp专题整理</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/09/读书笔记/拆掉思维里的墙(下)/">拆掉思维里的墙(下)</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/07/acm/7-7acm整理笔记/">acm7-7总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/30/读书笔记/拆掉思维里的墙(上)/">拆掉思维里的墙(上)</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/28/脚本/Script_ascii/">script： translate picture into ascii.text</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/27/读书笔记/哈佛的6堂独立思考课/">哈佛的6堂独立思考课</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/24/网页学习/djangoStart01/">Django_GO01</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/14/Linux-计算机基础认识/">Linux_计算机基础认识</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/10/python网络编程-init/">python网络编程.init()</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/07/随笔集/新的开始/">新的开始</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/01/07/网页学习/测试jsDOM笔记/">测试jsDOM入门笔记</a></li></ul><script></script></div><footer id="footer"><div class="outer"><div id="footer-info"><div class="footer-left"><i class="fa fa-copyright"></i> 2018 呼哧</div><div class="footer-right"><a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架">Hexo</a> Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题  v3.5">Yelee</a> by MOxFIVE <i class="fa fa-heart animated infinite pulse"></i></div></div><div class="visit"><span id="busuanzi_container_site_pv" style="display:none"><span id="site-visit" title="本站到访数"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span> </span></span><span>| </span><span id="busuanzi_container_page_pv" style="display:none"><span id="page-visit" title="本页阅读量"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span></span></span></div></div></footer></div><script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script><script>$(document).ready(function(){if(-1<window.navigator.userAgent.indexOf("iPad")||"none"===$(".left-col").css("display")){var a=["#9db3f4","#414141","#e5a859","#f5dfc6","#c084a0","#847e72","#cd8390","#996731"],c=Math.ceil(Math.random()*(a.length-1));$("body").css({"background-color":a[c],"background-size":"cover"})}else{var e="url(/background/bg-x.jpg)".replace(/x/gi,Math.ceil(5*Math.random()));$("body").css({background:e,"background-attachment":"fixed","background-size":"cover"})}})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});</script><script src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><div class="scroll" id="scroll"><a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a> <a href="#comments" onclick="load$hide()" title="查看评论"><i class="fa fa-comments-o"></i></a> <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a></div><script>var oOpenInNew={archives:".archive-article-title",miniArchives:"a.post-list-link",friends:"#js-friends a",socail:".social a"};for(var x in oOpenInNew)$(oOpenInNew[x]).attr("target","_blank")</script><script>var titleTime,originTitle=document.title;document.addEventListener("visibilitychange",function(){document.hidden?(document.title="(つェ⊂) 我藏好了哦~ "+originTitle,clearTimeout(titleTime)):(document.title="(*´∇｀*) 被你发现啦~ "+originTitle,titleTime=setTimeout(function(){document.title=originTitle},2e3))})</script><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body>